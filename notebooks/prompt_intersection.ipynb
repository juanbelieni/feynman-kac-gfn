{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28c6a6e-e051-4f13-93e9-a8156d7d17f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56dbf55897a44a6a644ebfd1e687e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 100,663,296 || all params: 3,921,742,848 || trainable%: 2.5668000147265135\n",
      "tokenizer.bos_token_id = 32011\n",
      "tokenizer.pad_token_id = 32012\n",
      "tokenizer.eos_token_id = 32000\n",
      "tokenizer.all_special_tokens = ['<|startoftext|>', '<|endoftext|>', '<unk>', '<|pad|>']\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch as t\n",
    "import einops\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=t.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=t.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=t.bfloat16,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    use_rslora=True,\n",
    ")\n",
    "\n",
    "lora_model = copy.deepcopy(base_model)\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", padding_side=\"left\")\n",
    "tokenizer.add_special_tokens({'bos_token': '<|startoftext|>'})\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "print(f\"{tokenizer.bos_token_id = }\")\n",
    "print(f\"{tokenizer.pad_token_id = }\")\n",
    "print(f\"{tokenizer.eos_token_id = }\")\n",
    "print(f\"{tokenizer.all_special_tokens = }\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7459b66b-a2e8-40c0-8ebf-b2fda346b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['someone like Lady Macb',\n",
      " 'Mary, but other women',\n",
      " 'the first person I met',\n",
      " 'Prof. Sir Timothy',\n",
      " 'Albert Einstein. I',\n",
      " 'Albert Einstein\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(prompt, **kwargs):\n",
    "    return tokenizer(prompt, return_tensors=\"pt\", padding=True, **kwargs).to(device)\n",
    "\n",
    "def get_mask(token_ids: t.Tensor,):\n",
    "    is_not_pad = (token_ids[:, :] != tokenizer.pad_token_id).bool()\n",
    "    \n",
    "    is_eos = (token_ids[:, :] == tokenizer.eos_token_id)\n",
    "    is_eos = (is_eos.cumsum(dim=-1) >= 1).bool()\n",
    "    is_not_eos = ~is_eos\n",
    "\n",
    "    mask = is_not_pad & is_not_eos\n",
    "\n",
    "    return mask.int()\n",
    "\n",
    "prompt_token_ids = tokenize([\n",
    "    \"My favorite woman is probably\",\n",
    "    \"My favorite scientist is probably\"]).input_ids\n",
    "prompt_masks = get_mask(prompt_token_ids)\n",
    "\n",
    "with t.inference_mode():\n",
    "    token_ids = lora_model.generate(\n",
    "        input_ids=prompt_token_ids,\n",
    "        attention_mask=prompt_masks,\n",
    "        max_new_tokens=5,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        temperature=1,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "pprint(tokenizer.batch_decode(token_ids[:, -5:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7c6e34-387c-4efd-a118-a9dab4d44c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def calc_log_rewards(\n",
    "    prompt_token_ids: t.Tensor,     # [n_prompts, n_tokens_prompts]\n",
    "    completion_token_ids: t.Tensor, # [n_completions, n_tokens_completion]\n",
    "    temperature: float = 1,\n",
    ") -> t.Tensor: # [n_completions]    \n",
    "    assert temperature > 0\n",
    "\n",
    "    n_prompts, n_tokens_prompts = prompt_token_ids.shape\n",
    "    n_completions, n_tokens_completion = completion_token_ids.shape\n",
    "\n",
    "    prompt_token_ids = prompt_token_ids.unsqueeze(1) # [n_prompts, 1, n_tokens_prompts]\n",
    "    completion_token_ids = completion_token_ids.unsqueeze(0) # [1, n_completions, n_tokens_completion]\n",
    "\n",
    "    prompt_token_ids = prompt_token_ids.expand(n_prompts, n_completions, n_tokens_prompts)\n",
    "    completion_token_ids = completion_token_ids.expand(n_prompts, n_completions, n_tokens_completion)\n",
    "\n",
    "    combined_token_ids = t.cat((prompt_token_ids, completion_token_ids), dim=2) # [n_prompts, n_completions, n_tokens_prompts + n_tokens_completion]\n",
    "    combined_token_ids = einops.rearrange(combined_token_ids, \"np nc nt -> (np nc) nt\")\n",
    "    combined_attention_mask = get_mask(combined_token_ids)\n",
    "\n",
    "    outputs = base_model(input_ids=combined_token_ids, attention_mask=combined_attention_mask)\n",
    "\n",
    "    logits = outputs.logits ** 1/temperature\n",
    "\n",
    "    log_probs = logits[:, :-1].log_softmax(dim=-1).gather(2, combined_token_ids[:, 1:, None])\n",
    "    log_probs = log_probs.squeeze(dim=2) * combined_attention_mask[:, 1:]\n",
    "    log_probs = log_probs[:, n_tokens_prompts-1:]\n",
    "    log_probs = einops.rearrange(log_probs, \"(np nc) nt -> np nc nt\", np=n_prompts, nc=n_completions)\n",
    "    log_rewards = log_probs.sum(dim=0)\n",
    "\n",
    "    eos_token_ids = t.full(combined_token_ids[:, :, None].shape, tokenizer.eos_token_id).to(device)\n",
    "    eos_log_probs = logits.log_softmax(dim=-1).gather(2, eos_token_ids)\n",
    "    eos_log_probs = eos_log_probs.squeeze(dim=1).squeeze(2) * combined_attention_mask\n",
    "    eos_log_probs = eos_log_probs[:, n_tokens_prompts-1:]\n",
    "    eos_log_probs = einops.rearrange(eos_log_probs, \"(np nc) nt -> np nc nt\", np=n_prompts, nc=n_completions)\n",
    "    eos_log_rewards = eos_log_probs.sum(dim=0)\n",
    "\n",
    "    return log_rewards, eos_log_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3732e5f-ce3e-4ab9-a9a4-cff1a4702952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_probs(\n",
    "    token_ids: t.Tensor, # [n_sentences, n_tokens]\n",
    "    temperature: float = 1,\n",
    "):\n",
    "    assert temperature > 0\n",
    "\n",
    "    n_sentences, n_tokens = token_ids.shape\n",
    "    attention_mask = get_mask(token_ids)\n",
    "\n",
    "    outputs = lora_model(input_ids=token_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits ** 1/temperature\n",
    "    \n",
    "    log_probs = logits[:, :-1].log_softmax(dim=-1).gather(2, token_ids[:, 1:, None])\n",
    "    log_probs = log_probs.squeeze(dim=1).squeeze(2) * attention_mask[:, :-1]\n",
    "\n",
    "    eos_token_ids = t.full(token_ids[:, :, None].shape, tokenizer.eos_token_id).to(device)\n",
    "    eos_log_probs = logits.log_softmax(dim=-1).gather(2, eos_token_ids)\n",
    "    eos_log_probs = eos_log_probs.squeeze(dim=1).squeeze(2) * attention_mask\n",
    "\n",
    "    return log_probs, eos_log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60ef8fc-1d57-4ba4-8445-3b103c67be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Z = t.tensor(-1.0, requires_grad=True)\n",
    "\n",
    "optimizer_Z = t.optim.AdamW([\n",
    "    log_Z\n",
    "], lr=0.1)\n",
    "\n",
    "optimizer_lora = t.optim.AdamW([\n",
    "    *[v for k, v in lora_model.named_parameters() if \"lora\" in k],\n",
    "], lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac110df2-2b89-4a89-8e30-57005b8766ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00: loss = 615.717, Z = 0.333\n",
      "01: loss = 300.777, Z = 0.303\n",
      "02: loss = 154.256, Z = 0.281\n",
      "03: loss = 139.499, Z = 0.261\n",
      "04: loss = 92.526, Z = 0.244\n",
      "Updating model\n",
      "05: loss = 88.012, Z = 0.232\n",
      "06: loss = 49.141, Z = 0.223\n",
      "07: loss = 35.153, Z = 0.214\n",
      "08: loss = 32.487, Z = 0.207\n",
      "09: loss = 27.923, Z = 0.200\n",
      "Updating model\n",
      "10: loss = 20.634, Z = 0.193\n",
      "11: loss = 18.400, Z = 0.189\n",
      "12: loss = 17.193, Z = 0.185\n",
      "13: loss = 17.004, Z = 0.181\n",
      "14: loss = 17.526, Z = 0.178\n",
      "Updating model\n",
      "15: loss = 16.783, Z = 0.175\n",
      "16: loss = 16.046, Z = 0.172\n",
      "17: loss = 15.765, Z = 0.169\n",
      "18: loss = 10.702, Z = 0.167\n",
      "19: loss = 11.622, Z = 0.165\n",
      "Updating model\n",
      "20: loss = 11.410, Z = 0.164\n",
      "21: loss = 9.079, Z = 0.162\n",
      "22: loss = 9.142, Z = 0.162\n",
      "23: loss = 8.358, Z = 0.161\n",
      "24: loss = 6.428, Z = 0.160\n",
      "Updating model\n",
      "25: loss = 5.881, Z = 0.159\n",
      "26: loss = 5.469, Z = 0.158\n",
      "27: loss = 3.891, Z = 0.157\n",
      "28: loss = 5.830, Z = 0.156\n",
      "29: loss = 3.222, Z = 0.155\n",
      "Updating model\n",
      "30: loss = 3.372, Z = 0.155\n",
      "31: loss = 4.657, Z = 0.155\n",
      "32: loss = 5.544, Z = 0.154\n",
      "33: loss = 5.410, Z = 0.154\n",
      "34: loss = 5.226, Z = 0.154\n",
      "Updating model\n",
      "35: loss = 7.692, Z = 0.154\n",
      "36: loss = 4.839, Z = 0.153\n",
      "37: loss = 5.077, Z = 0.153\n",
      "38: loss = 4.278, Z = 0.153\n",
      "39: loss = 3.899, Z = 0.153\n",
      "Updating model\n",
      "40: loss = 2.998, Z = 0.153\n",
      "41: loss = 3.383, Z = 0.152\n",
      "42: loss = 3.337, Z = 0.152\n",
      "43: loss = 3.381, Z = 0.152\n",
      "44: loss = 3.656, Z = 0.153\n",
      "Updating model\n",
      "45: loss = 2.549, Z = 0.153\n",
      "46: loss = 3.085, Z = 0.153\n",
      "47: loss = 2.246, Z = 0.153\n",
      "48: loss = 2.732, Z = 0.153\n",
      "49: loss = 2.214, Z = 0.153\n",
      "Updating model\n",
      "50: loss = 1.652, Z = 0.153\n",
      "51: loss = 1.596, Z = 0.153\n",
      "52: loss = 1.527, Z = 0.153\n",
      "53: loss = 1.809, Z = 0.153\n",
      "54: loss = 2.327, Z = 0.153\n",
      "Updating model\n",
      "55: loss = 1.768, Z = 0.154\n",
      "56: loss = 1.938, Z = 0.154\n",
      "57: loss = 2.077, Z = 0.154\n",
      "58: loss = 1.109, Z = 0.154\n",
      "59: loss = 1.833, Z = 0.154\n",
      "Updating model\n",
      "60: loss = 1.888, Z = 0.155\n",
      "61: loss = 1.571, Z = 0.155\n",
      "62: loss = 1.362, Z = 0.155\n",
      "63: loss = 1.691, Z = 0.155\n",
      "64: loss = 1.660, Z = 0.155\n",
      "Updating model\n",
      "65: loss = 1.732, Z = 0.155\n",
      "66: loss = 1.509, Z = 0.156\n",
      "67: loss = 1.414, Z = 0.156\n",
      "68: loss = 1.380, Z = 0.156\n",
      "69: loss = 1.812, Z = 0.156\n",
      "Updating model\n",
      "70: loss = 1.569, Z = 0.157\n",
      "71: loss = 1.427, Z = 0.157\n",
      "72: loss = 1.441, Z = 0.157\n",
      "73: loss = 1.707, Z = 0.157\n",
      "74: loss = 1.219, Z = 0.158\n",
      "Updating model\n",
      "75: loss = 1.084, Z = 0.158\n",
      "76: loss = 1.618, Z = 0.158\n",
      "77: loss = 1.619, Z = 0.158\n",
      "78: loss = 1.503, Z = 0.158\n",
      "79: loss = 1.349, Z = 0.159\n",
      "Updating model\n",
      "80: loss = 1.949, Z = 0.159\n",
      "81: loss = 1.366, Z = 0.159\n",
      "82: loss = 2.128, Z = 0.159\n",
      "83: loss = 1.306, Z = 0.160\n",
      "84: loss = 1.799, Z = 0.160\n",
      "Updating model\n",
      "85: loss = 1.590, Z = 0.160\n",
      "86: loss = 1.701, Z = 0.160\n",
      "87: loss = 1.297, Z = 0.161\n",
      "88: loss = 1.388, Z = 0.161\n",
      "89: loss = 1.496, Z = 0.161\n",
      "Updating model\n",
      "90: loss = 1.100, Z = 0.162\n",
      "91: loss = 1.387, Z = 0.162\n",
      "92: loss = 1.349, Z = 0.162\n",
      "93: loss = 1.591, Z = 0.162\n",
      "94: loss = 1.219, Z = 0.163\n",
      "Updating model\n",
      "95: loss = 0.809, Z = 0.163\n",
      "96: loss = 0.825, Z = 0.163\n",
      "97: loss = 1.073, Z = 0.163\n",
      "98: loss = 1.130, Z = 0.164\n",
      "99: loss = 1.442, Z = 0.164\n",
      "Updating model\n",
      "100: loss = 1.274, Z = 0.164\n",
      "101: loss = 1.241, Z = 0.164\n",
      "102: loss = 1.421, Z = 0.165\n",
      "103: loss = 1.254, Z = 0.165\n",
      "104: loss = 0.952, Z = 0.165\n",
      "Updating model\n",
      "105: loss = 1.136, Z = 0.165\n",
      "106: loss = 1.051, Z = 0.166\n",
      "107: loss = 0.874, Z = 0.166\n",
      "108: loss = 1.211, Z = 0.166\n",
      "109: loss = 0.989, Z = 0.166\n",
      "Updating model\n",
      "110: loss = 1.187, Z = 0.167\n",
      "111: loss = 1.149, Z = 0.167\n",
      "112: loss = 0.898, Z = 0.167\n",
      "113: loss = 0.935, Z = 0.167\n",
      "114: loss = 0.892, Z = 0.168\n",
      "Updating model\n",
      "115: loss = 0.953, Z = 0.168\n",
      "116: loss = 1.529, Z = 0.168\n",
      "117: loss = 1.349, Z = 0.168\n",
      "118: loss = 1.518, Z = 0.169\n",
      "119: loss = 0.637, Z = 0.169\n",
      "Updating model\n",
      "120: loss = 0.797, Z = 0.169\n",
      "121: loss = 1.120, Z = 0.170\n",
      "122: loss = 1.069, Z = 0.170\n",
      "123: loss = 1.006, Z = 0.170\n",
      "124: loss = 0.960, Z = 0.170\n",
      "Updating model\n",
      "125: loss = 0.951, Z = 0.171\n",
      "126: loss = 1.284, Z = 0.171\n",
      "127: loss = 0.966, Z = 0.171\n",
      "128: loss = 0.700, Z = 0.171\n",
      "129: loss = 1.060, Z = 0.172\n",
      "Updating model\n",
      "130: loss = 0.726, Z = 0.172\n",
      "131: loss = 1.011, Z = 0.172\n",
      "132: loss = 0.558, Z = 0.173\n",
      "133: loss = 0.703, Z = 0.173\n",
      "134: loss = 1.271, Z = 0.173\n",
      "Updating model\n",
      "135: loss = 0.737, Z = 0.173\n",
      "136: loss = 0.739, Z = 0.173\n",
      "137: loss = 0.934, Z = 0.174\n",
      "138: loss = 0.825, Z = 0.174\n",
      "139: loss = 0.676, Z = 0.174\n",
      "Updating model\n",
      "140: loss = 0.816, Z = 0.175\n",
      "141: loss = 0.848, Z = 0.175\n",
      "142: loss = 0.785, Z = 0.175\n",
      "143: loss = 0.860, Z = 0.176\n",
      "144: loss = 0.755, Z = 0.176\n",
      "Updating model\n",
      "145: loss = 1.004, Z = 0.176\n",
      "146: loss = 0.805, Z = 0.176\n",
      "147: loss = 0.676, Z = 0.177\n",
      "148: loss = 0.797, Z = 0.177\n",
      "149: loss = 0.533, Z = 0.177\n",
      "Updating model\n",
      "150: loss = 0.769, Z = 0.178\n",
      "151: loss = 0.971, Z = 0.178\n",
      "152: loss = 0.748, Z = 0.178\n",
      "153: loss = 0.859, Z = 0.178\n",
      "154: loss = 0.695, Z = 0.179\n",
      "Updating model\n",
      "155: loss = 0.625, Z = 0.179\n",
      "156: loss = 0.393, Z = 0.179\n",
      "157: loss = 1.247, Z = 0.179\n",
      "158: loss = 0.833, Z = 0.180\n",
      "159: loss = 0.869, Z = 0.180\n",
      "Updating model\n",
      "160: loss = 0.846, Z = 0.180\n",
      "161: loss = 0.863, Z = 0.181\n",
      "162: loss = 0.813, Z = 0.181\n",
      "163: loss = 0.527, Z = 0.181\n",
      "164: loss = 0.784, Z = 0.181\n",
      "Updating model\n",
      "165: loss = 0.923, Z = 0.182\n",
      "166: loss = 0.932, Z = 0.182\n",
      "167: loss = 0.894, Z = 0.182\n",
      "168: loss = 1.074, Z = 0.182\n",
      "169: loss = 0.559, Z = 0.183\n",
      "Updating model\n",
      "170: loss = 0.685, Z = 0.183\n",
      "171: loss = 0.765, Z = 0.183\n",
      "172: loss = 0.512, Z = 0.184\n",
      "173: loss = 0.932, Z = 0.184\n",
      "174: loss = 0.637, Z = 0.184\n",
      "Updating model\n",
      "175: loss = 0.710, Z = 0.184\n",
      "176: loss = 0.893, Z = 0.185\n",
      "177: loss = 0.724, Z = 0.185\n",
      "178: loss = 0.716, Z = 0.185\n",
      "179: loss = 0.590, Z = 0.185\n",
      "Updating model\n",
      "180: loss = 1.036, Z = 0.186\n",
      "181: loss = 0.630, Z = 0.186\n",
      "182: loss = 0.904, Z = 0.186\n",
      "183: loss = 0.538, Z = 0.186\n",
      "184: loss = 0.540, Z = 0.187\n",
      "Updating model\n",
      "185: loss = 0.701, Z = 0.187\n",
      "186: loss = 0.535, Z = 0.187\n",
      "187: loss = 0.776, Z = 0.188\n",
      "188: loss = 0.694, Z = 0.188\n",
      "189: loss = 0.508, Z = 0.188\n",
      "Updating model\n",
      "190: loss = 0.551, Z = 0.188\n",
      "191: loss = 1.140, Z = 0.189\n",
      "192: loss = 0.837, Z = 0.189\n",
      "193: loss = 0.300, Z = 0.189\n",
      "194: loss = 0.533, Z = 0.190\n",
      "Updating model\n",
      "195: loss = 0.729, Z = 0.190\n",
      "196: loss = 0.635, Z = 0.190\n",
      "197: loss = 0.492, Z = 0.191\n",
      "198: loss = 0.903, Z = 0.191\n",
      "199: loss = 0.756, Z = 0.191\n",
      "Updating model\n"
     ]
    }
   ],
   "source": [
    "n_sentences = 500\n",
    "max_len = 5\n",
    "\n",
    "lora_model.train()\n",
    "\n",
    "lora_model_copy = copy.deepcopy(lora_model)\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer_Z.zero_grad()\n",
    "    optimizer_lora.zero_grad()\n",
    "\n",
    "    with t.no_grad():\n",
    "        token_ids = lora_model.generate(\n",
    "            input_ids=prompt_token_ids[:1],\n",
    "            attention_mask=prompt_masks[:1],\n",
    "            max_new_tokens=max_len,\n",
    "            do_sample=True,\n",
    "            temperature=1.25,\n",
    "            num_return_sequences=n_sentences,            \n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        log_rewards, eos_log_rewards = calc_log_rewards(\n",
    "            prompt_token_ids=prompt_token_ids,\n",
    "            completion_token_ids=token_ids[:, -max_len:],\n",
    "            temperature=1.25,\n",
    "        )\n",
    "\n",
    "    log_probs, eos_log_probs = calc_log_probs(\n",
    "        token_ids=token_ids,\n",
    "        temperature=1,\n",
    "    )\n",
    "\n",
    "    log_probs = log_probs[:, -max_len:]\n",
    "    eos_log_probs = eos_log_probs[:, -max_len-1:]\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    for i in range(0, max_len + 1):\n",
    "        sub_loss = (log_Z\n",
    "                    + log_probs[:, :i].sum(-1)\n",
    "                    + eos_log_probs[:, i]\n",
    "                    - log_rewards[:, :i].sum(-1)\n",
    "                    - eos_log_rewards[:, i]) ** 2\n",
    "\n",
    "        loss += sub_loss\n",
    "\n",
    "    loss = loss.mean() / (max_len + 1)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_Z.step()\n",
    "    optimizer_lora.step()\n",
    "\n",
    "    print(f\"{epoch:02d}: loss = {loss.item():.3f}, Z = {log_Z.exp().item():.3f}\")\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"Updating model\")\n",
    "        lora_model_copy = copy.deepcopy(lora_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d374a638-0741-4e2f-97c3-df6965459e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My favorite woman is probably Marie Curie. She',\n",
       " 'My favorite woman is probably Marie Curie. She',\n",
       " 'My favorite woman is probably Albert Einstein. He',\n",
       " 'My favorite woman is probably Marie Curie. Unfortunately',\n",
       " 'My favorite woman is probably Marie Curie. She',\n",
       " \"My favorite woman is probably my mother. She'\",\n",
       " 'My favorite woman is probably Ada Lovelace.',\n",
       " 'My favorite woman is probably Marie Curie. I',\n",
       " 'My favorite woman is probably Marie Curie. She',\n",
       " 'My favorite woman is probably Albert Einstein. He']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.eval()\n",
    "\n",
    "with t.inference_mode():\n",
    "    token_ids = lora_model.generate(\n",
    "        input_ids=prompt_token_ids[:1],\n",
    "        attention_mask=prompt_masks[:1],\n",
    "        max_new_tokens=max_len,\n",
    "        num_return_sequences=10,\n",
    "        do_sample=True,\n",
    "        temperature=1,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
